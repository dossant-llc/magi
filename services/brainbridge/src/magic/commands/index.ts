/**
 * mAGIc Index Command
 * Builds vector index for semantic search using embeddings
 */

import * as fs from 'fs/promises';
import * as path from 'path';
import { EmbeddingService } from '../../services/embedding-service';
import { LoggerService } from '../../services/logger-service';

interface IndexOptions {
  force?: boolean;
}

export async function indexCommand(options: IndexOptions) {
  console.log('ğŸ” mAGIc Index: Building vector embeddings index...');
  
  try {
    // Initialize services  
    const loggerService = new LoggerService(path.join(process.cwd(), 'logs', 'magic-index.log'));
    const embeddingService = new EmbeddingService(loggerService);
    
    console.log('ğŸ¤– Initializing embedding service with mxbai-embed-large...');
    
    // Build embeddings index
    const stats = await embeddingService.buildIndex(options.force || false);
    
    console.log(`âœ… Index built successfully!`);
    console.log(`ğŸ“Š Processed: ${stats.processed} files`);
    console.log(`â­ï¸  Skipped: ${stats.skipped} files (unchanged)`);
    
    if (stats.errors > 0) {
      console.log(`âš ï¸  Errors: ${stats.errors} files failed to process`);
    }
    
    console.log(`ğŸ’¾ Vector embeddings saved to .index/embeddings.json`);
    
    if (stats.processed === 0 && stats.skipped === 0) {
      console.log('\nğŸ’¡ Tip: Add some memories first!');
      console.log('   magic save "Your first memory content here"');
    } else if (options.force) {
      console.log('\nğŸ”„ Force rebuild completed - all embeddings regenerated');
    } else {
      console.log('\nâš¡ Incremental update - only new/changed files processed');
    }
    
  } catch (error) {
    console.error('âŒ Error building embeddings index:', error);
    console.error('   Make sure Ollama is running with mxbai-embed-large model');
    console.error('   Run: ollama pull mxbai-embed-large');
    process.exit(1);
  }
}